	vpbroadcastd	.LCPI6_0(%rip), %ymm9   # ymm9 = [1,1,1,1,1,1,1,1]
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm0, %ymm0
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm4, %ymm4
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm5, %ymm5
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm6, %ymm6
	vpaddd	%ymm0, %ymm4, %ymm0
	vpaddd	%ymm0, %ymm5, %ymm0
	vpaddd	%ymm0, %ymm6, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovups	(%rsi,%rax,2), %ymm0
	vmovups	32(%rsi,%rax,2), %ymm1
	vmovups	64(%rsi,%rax,2), %ymm2
	vmovups	96(%rsi,%rax,2), %ymm3
	vxorps	(%r8,%rax,2), %ymm0, %ymm0
	vxorps	32(%r8,%rax,2), %ymm1, %ymm1
	vxorps	64(%r8,%rax,2), %ymm2, %ymm2
	vxorps	96(%r8,%rax,2), %ymm3, %ymm3
	vmovups	%ymm0, (%rsi,%rax,2)
	vmovups	%ymm1, 32(%rsi,%rax,2)
	vmovups	%ymm2, 64(%rsi,%rax,2)
	vmovups	%ymm3, 96(%rsi,%rax,2)
	vmovups	1040(%rsp), %ymm0
	vmovups	1072(%rsp), %ymm1
	vmovups	1104(%rsp), %ymm2
	vmovups	1136(%rsp), %ymm3
	vxorps	2072(%rsp), %ymm0, %ymm0
	vxorps	2104(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 8(%rsp)
	vmovups	%ymm1, 40(%rsp)
	vxorps	2136(%rsp), %ymm2, %ymm0
	vxorps	2168(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 72(%rsp)
	vmovups	%ymm1, 104(%rsp)
	vmovups	1168(%rsp), %ymm0
	vmovups	1200(%rsp), %ymm1
	vxorps	2200(%rsp), %ymm0, %ymm0
	vxorps	2232(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 136(%rsp)
	vmovups	%ymm1, 168(%rsp)
	vmovups	1232(%rsp), %ymm0
	vmovups	1264(%rsp), %ymm1
	vmovups	1296(%rsp), %ymm2
	vmovups	1328(%rsp), %ymm3
	vxorps	2264(%rsp), %ymm0, %ymm0
	vxorps	2296(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 200(%rsp)
	vmovups	%ymm1, 232(%rsp)
	vxorps	2328(%rsp), %ymm2, %ymm0
	vxorps	2360(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 264(%rsp)
	vmovups	%ymm1, 296(%rsp)
	vmovups	1360(%rsp), %ymm0
	vmovups	1392(%rsp), %ymm1
	vxorps	2392(%rsp), %ymm0, %ymm0
	vxorps	2424(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 328(%rsp)
	vmovups	%ymm1, 360(%rsp)
	vmovups	1424(%rsp), %ymm0
	vmovups	1456(%rsp), %ymm1
	vxorps	2456(%rsp), %ymm0, %ymm0
	vxorps	2488(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 392(%rsp)
	vmovups	%ymm1, 424(%rsp)
	vmovups	1488(%rsp), %ymm0
	vmovups	1520(%rsp), %ymm1
	vxorps	2520(%rsp), %ymm0, %ymm0
	vxorps	2552(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 456(%rsp)
	vmovups	%ymm1, 488(%rsp)
	vmovups	1552(%rsp), %ymm0
	vmovups	1584(%rsp), %ymm1
	vxorps	2584(%rsp), %ymm0, %ymm0
	vxorps	2616(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 520(%rsp)
	vmovups	%ymm1, 552(%rsp)
	vmovups	1616(%rsp), %ymm0
	vmovups	1648(%rsp), %ymm1
	vxorps	2648(%rsp), %ymm0, %ymm0
	vxorps	2680(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 584(%rsp)
	vmovups	%ymm1, 616(%rsp)
	vmovups	1680(%rsp), %ymm0
	vmovups	1712(%rsp), %ymm1
	vxorps	2712(%rsp), %ymm0, %ymm0
	vxorps	2744(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 648(%rsp)
	vmovups	%ymm1, 680(%rsp)
	vmovups	1744(%rsp), %ymm0
	vmovups	1776(%rsp), %ymm1
	vxorps	2776(%rsp), %ymm0, %ymm0
	vxorps	2808(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 712(%rsp)
	vmovups	%ymm1, 744(%rsp)
	vmovups	1808(%rsp), %ymm0
	vmovups	1840(%rsp), %ymm1
	vxorps	2840(%rsp), %ymm0, %ymm0
	vxorps	2872(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 776(%rsp)
	vmovups	%ymm1, 808(%rsp)
	vmovups	1872(%rsp), %ymm0
	vmovups	1904(%rsp), %ymm1
	vxorps	2904(%rsp), %ymm0, %ymm0
	vxorps	2936(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 840(%rsp)
	vmovups	%ymm1, 872(%rsp)
	vmovups	1936(%rsp), %ymm0
	vmovups	1968(%rsp), %ymm1
	vxorps	2968(%rsp), %ymm0, %ymm0
	vxorps	3000(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 904(%rsp)
	vmovups	%ymm1, 936(%rsp)
	vmovups	2000(%rsp), %ymm0
	vmovups	2032(%rsp), %ymm1
	vxorps	3032(%rsp), %ymm0, %ymm0
	vxorps	3064(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 968(%rsp)
	vmovups	%ymm1, 1000(%rsp)
	vpbroadcastd	.LCPI14_0(%rip), %ymm9  # ymm9 = [1,1,1,1,1,1,1,1]
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm0, %ymm0
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm4, %ymm4
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm5, %ymm5
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm6, %ymm6
	vpaddd	%ymm0, %ymm4, %ymm0
	vpaddd	%ymm0, %ymm5, %ymm0
	vpaddd	%ymm0, %ymm6, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovups	3104(%rsp), %ymm0
	vmovups	3136(%rsp), %ymm1
	vmovups	3168(%rsp), %ymm2
	vmovups	3200(%rsp), %ymm3
	vxorps	4136(%rsp), %ymm0, %ymm0
	vxorps	4168(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2072(%rsp)
	vmovups	%ymm1, 2104(%rsp)
	vxorps	4200(%rsp), %ymm2, %ymm0
	vxorps	4232(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 2136(%rsp)
	vmovups	%ymm1, 2168(%rsp)
	vmovups	3232(%rsp), %ymm0
	vmovups	3264(%rsp), %ymm1
	vxorps	4264(%rsp), %ymm0, %ymm0
	vxorps	4296(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2200(%rsp)
	vmovups	%ymm1, 2232(%rsp)
	vmovups	3296(%rsp), %ymm0
	vmovups	3328(%rsp), %ymm1
	vmovups	3360(%rsp), %ymm2
	vmovups	3392(%rsp), %ymm3
	vxorps	4328(%rsp), %ymm0, %ymm0
	vxorps	4360(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2264(%rsp)
	vmovups	%ymm1, 2296(%rsp)
	vxorps	4392(%rsp), %ymm2, %ymm0
	vxorps	4424(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 2328(%rsp)
	vmovups	%ymm1, 2360(%rsp)
	vmovups	3424(%rsp), %ymm0
	vmovups	3456(%rsp), %ymm1
	vxorps	4456(%rsp), %ymm0, %ymm0
	vxorps	4488(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2392(%rsp)
	vmovups	%ymm1, 2424(%rsp)
	vmovups	3488(%rsp), %ymm0
	vmovups	3520(%rsp), %ymm1
	vxorps	4520(%rsp), %ymm0, %ymm0
	vxorps	4552(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2456(%rsp)
	vmovups	%ymm1, 2488(%rsp)
	vmovups	3552(%rsp), %ymm0
	vmovups	3584(%rsp), %ymm1
	vxorps	4584(%rsp), %ymm0, %ymm0
	vxorps	4616(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2520(%rsp)
	vmovups	%ymm1, 2552(%rsp)
	vmovups	3616(%rsp), %ymm0
	vmovups	3648(%rsp), %ymm1
	vxorps	4648(%rsp), %ymm0, %ymm0
	vxorps	4680(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2584(%rsp)
	vmovups	%ymm1, 2616(%rsp)
	vmovups	3680(%rsp), %ymm0
	vmovups	3712(%rsp), %ymm1
	vxorps	4712(%rsp), %ymm0, %ymm0
	vxorps	4744(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2648(%rsp)
	vmovups	%ymm1, 2680(%rsp)
	vmovups	3744(%rsp), %ymm0
	vmovups	3776(%rsp), %ymm1
	vxorps	4776(%rsp), %ymm0, %ymm0
	vxorps	4808(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2712(%rsp)
	vmovups	%ymm1, 2744(%rsp)
	vmovups	3808(%rsp), %ymm0
	vmovups	3840(%rsp), %ymm1
	vxorps	4840(%rsp), %ymm0, %ymm0
	vxorps	4872(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2776(%rsp)
	vmovups	%ymm1, 2808(%rsp)
	vmovups	3872(%rsp), %ymm0
	vmovups	3904(%rsp), %ymm1
	vxorps	4904(%rsp), %ymm0, %ymm0
	vxorps	4936(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2840(%rsp)
	vmovups	%ymm1, 2872(%rsp)
	vmovups	3936(%rsp), %ymm0
	vmovups	3968(%rsp), %ymm1
	vxorps	4968(%rsp), %ymm0, %ymm0
	vxorps	5000(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2904(%rsp)
	vmovups	%ymm1, 2936(%rsp)
	vmovups	4000(%rsp), %ymm0
	vmovups	4032(%rsp), %ymm1
	vxorps	5032(%rsp), %ymm0, %ymm0
	vxorps	5064(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2968(%rsp)
	vmovups	%ymm1, 3000(%rsp)
	vmovups	4064(%rsp), %ymm0
	vmovups	4096(%rsp), %ymm1
	vxorps	5096(%rsp), %ymm0, %ymm0
	vxorps	5128(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 3032(%rsp)
	vmovups	%ymm1, 3064(%rsp)
	vmovups	32(%rsp,%rsi,2), %ymm0
	vmovups	64(%rsp,%rsi,2), %ymm1
	vmovups	96(%rsp,%rsi,2), %ymm2
	vmovups	128(%rsp,%rsi,2), %ymm3
	vxorps	1064(%rsp,%rsi,2), %ymm0, %ymm0
	vxorps	1096(%rsp,%rsi,2), %ymm1, %ymm1
	vmovups	%ymm0, 32(%rsp,%rsi,2)
	vmovups	%ymm1, 64(%rsp,%rsi,2)
	vxorps	1128(%rsp,%rsi,2), %ymm2, %ymm0
	vxorps	1160(%rsp,%rsi,2), %ymm3, %ymm1
	vmovups	%ymm0, 96(%rsp,%rsi,2)
	vmovups	%ymm1, 128(%rsp,%rsi,2)
	vmovups	32(%rsp,%rsi,2), %ymm0
	vmovups	64(%rsp,%rsi,2), %ymm1
	vxorps	1064(%rsp,%rsi,2), %ymm0, %ymm0
	vxorps	1096(%rsp,%rsi,2), %ymm1, %ymm1
	vmovups	%ymm0, 32(%rsp,%rsi,2)
	vmovups	%ymm1, 64(%rsp,%rsi,2)
	vmovups	3144(%rsp), %ymm0
	vmovups	3176(%rsp), %ymm1
	vmovups	3208(%rsp), %ymm2
	vmovups	3240(%rsp), %ymm3
	vxorps	4176(%rsp), %ymm0, %ymm0
	vxorps	4208(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2112(%rsp)
	vmovups	%ymm1, 2144(%rsp)
	vxorps	4240(%rsp), %ymm2, %ymm0
	vxorps	4272(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 2176(%rsp)
	vmovups	%ymm1, 2208(%rsp)
	vmovups	3272(%rsp), %ymm0
	vmovups	3304(%rsp), %ymm1
	vxorps	4304(%rsp), %ymm0, %ymm0
	vxorps	4336(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2240(%rsp)
	vmovups	%ymm1, 2272(%rsp)
	vmovups	3336(%rsp), %ymm0
	vmovups	3368(%rsp), %ymm1
	vmovdqu	3400(%rsp), %ymm2
	vmovdqu	3432(%rsp), %ymm3
	vxorps	4368(%rsp), %ymm0, %ymm0
	vxorps	4400(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2304(%rsp)
	vmovups	%ymm1, 2336(%rsp)
	vpxor	4432(%rsp), %ymm2, %ymm0
	vpxor	4464(%rsp), %ymm3, %ymm1
	vmovdqu	%ymm0, 2368(%rsp)
	vmovdqu	%ymm1, 2400(%rsp)
	vmovups	3464(%rsp), %ymm0
	vmovups	3496(%rsp), %ymm1
	vxorps	4496(%rsp), %ymm0, %ymm0
	vxorps	4528(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2432(%rsp)
	vmovups	%ymm1, 2464(%rsp)
	vmovups	3528(%rsp), %ymm0
	vmovups	3560(%rsp), %ymm1
	vxorps	4560(%rsp), %ymm0, %ymm0
	vxorps	4592(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2496(%rsp)
	vmovups	%ymm1, 2528(%rsp)
	vmovups	3592(%rsp), %ymm0
	vmovups	3624(%rsp), %ymm1
	vxorps	4624(%rsp), %ymm0, %ymm0
	vxorps	4656(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2560(%rsp)
	vmovups	%ymm1, 2592(%rsp)
	vmovups	3656(%rsp), %ymm0
	vmovups	3688(%rsp), %ymm1
	vxorps	4688(%rsp), %ymm0, %ymm0
	vxorps	4720(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2624(%rsp)
	vmovups	%ymm1, 2656(%rsp)
	vmovups	3720(%rsp), %ymm0
	vmovups	3752(%rsp), %ymm1
	vxorps	4752(%rsp), %ymm0, %ymm0
	vxorps	4784(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2688(%rsp)
	vmovups	%ymm1, 2720(%rsp)
	vmovups	3784(%rsp), %ymm0
	vmovups	3816(%rsp), %ymm1
	vxorps	4816(%rsp), %ymm0, %ymm0
	vxorps	4848(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2752(%rsp)
	vmovups	%ymm1, 2784(%rsp)
	vmovups	3848(%rsp), %ymm0
	vmovups	3880(%rsp), %ymm1
	vxorps	4880(%rsp), %ymm0, %ymm0
	vxorps	4912(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2816(%rsp)
	vmovups	%ymm1, 2848(%rsp)
	vmovups	3912(%rsp), %ymm0
	vmovups	3944(%rsp), %ymm1
	vxorps	4944(%rsp), %ymm0, %ymm0
	vxorps	4976(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2880(%rsp)
	vmovups	%ymm1, 2912(%rsp)
	vmovups	3976(%rsp), %ymm0
	vmovups	4008(%rsp), %ymm1
	vxorps	5008(%rsp), %ymm0, %ymm0
	vxorps	5040(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2944(%rsp)
	vmovups	%ymm1, 2976(%rsp)
	vmovups	4040(%rsp), %ymm0
	vmovups	4072(%rsp), %ymm1
	vxorps	5072(%rsp), %ymm0, %ymm0
	vxorps	5104(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 3008(%rsp)
	vmovups	%ymm1, 3040(%rsp)
	vmovdqu	4104(%rsp), %ymm0
	vmovdqu	4136(%rsp), %ymm1
	vpxor	5136(%rsp), %ymm0, %ymm0
	vpxor	5168(%rsp), %ymm1, %ymm1
	vmovdqu	%ymm0, 3072(%rsp)
	vmovdqu	%ymm1, 3104(%rsp)
	vpbroadcastd	.LCPI26_0(%rip), %ymm9  # ymm9 = [1,1,1,1,1,1,1,1]
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm0, %ymm0
	vpmovzxwd	%xmm7, %ymm7            # ymm7 = xmm7[0],zero,xmm7[1],zero,xmm7[2],zero,xmm7[3],zero,xmm7[4],zero,xmm7[5],zero,xmm7[6],zero,xmm7[7],zero
	vpand	%ymm7, %ymm9, %ymm7
	vpaddd	%ymm7, %ymm4, %ymm4
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm5, %ymm5
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpand	%ymm2, %ymm9, %ymm2
	vpaddd	%ymm2, %ymm6, %ymm6
	vpaddd	%ymm0, %ymm4, %ymm0
	vpaddd	%ymm0, %ymm5, %ymm0
	vpaddd	%ymm0, %ymm6, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vpshufd	$27, (%rsi,%rbp,4), %ymm0       # ymm0 = mem[3,2,1,0,7,6,5,4]
	vpermq	$78, %ymm0, %ymm0               # ymm0 = ymm0[2,3,0,1]
	vpshufd	$27, -32(%rsi,%rbp,4), %ymm1    # ymm1 = mem[3,2,1,0,7,6,5,4]
	vpshufd	$27, -64(%rsi,%rbp,4), %ymm2    # ymm2 = mem[3,2,1,0,7,6,5,4]
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vpermq	$78, %ymm2, %ymm2               # ymm2 = ymm2[2,3,0,1]
	vpshufd	$27, -96(%rsi,%rbp,4), %ymm3    # ymm3 = mem[3,2,1,0,7,6,5,4]
	vpermq	$78, %ymm3, %ymm3               # ymm3 = ymm3[2,3,0,1]
	vmovdqu	%ymm0, -96(%rdx)
	vmovdqu	%ymm1, -64(%rdx)
	vmovdqu	%ymm2, -32(%rdx)
	vmovdqu	%ymm3, (%rdx)
	vmovups	3120(%rsp), %ymm0
	vmovups	3152(%rsp), %ymm1
	vmovups	3184(%rsp), %ymm2
	vmovups	3216(%rsp), %ymm3
	vxorps	4152(%rsp), %ymm0, %ymm0
	vxorps	4184(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2088(%rsp)
	vmovups	%ymm1, 2120(%rsp)
	vxorps	4216(%rsp), %ymm2, %ymm0
	vxorps	4248(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 2152(%rsp)
	vmovups	%ymm1, 2184(%rsp)
	vmovups	3248(%rsp), %ymm0
	vmovups	3280(%rsp), %ymm1
	vxorps	4280(%rsp), %ymm0, %ymm0
	vxorps	4312(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2216(%rsp)
	vmovups	%ymm1, 2248(%rsp)
	vmovups	3312(%rsp), %ymm0
	vmovups	3344(%rsp), %ymm1
	vmovups	3376(%rsp), %ymm2
	vmovups	3408(%rsp), %ymm3
	vxorps	4344(%rsp), %ymm0, %ymm0
	vxorps	4376(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2280(%rsp)
	vmovups	%ymm1, 2312(%rsp)
	vxorps	4408(%rsp), %ymm2, %ymm0
	vxorps	4440(%rsp), %ymm3, %ymm1
	vmovups	%ymm0, 2344(%rsp)
	vmovups	%ymm1, 2376(%rsp)
	vmovups	3440(%rsp), %ymm0
	vmovups	3472(%rsp), %ymm1
	vxorps	4472(%rsp), %ymm0, %ymm0
	vxorps	4504(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2408(%rsp)
	vmovups	%ymm1, 2440(%rsp)
	vmovups	3504(%rsp), %ymm0
	vmovups	3536(%rsp), %ymm1
	vxorps	4536(%rsp), %ymm0, %ymm0
	vxorps	4568(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2472(%rsp)
	vmovups	%ymm1, 2504(%rsp)
	vmovups	3568(%rsp), %ymm0
	vmovups	3600(%rsp), %ymm1
	vxorps	4600(%rsp), %ymm0, %ymm0
	vxorps	4632(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2536(%rsp)
	vmovups	%ymm1, 2568(%rsp)
	vmovups	3632(%rsp), %ymm0
	vmovups	3664(%rsp), %ymm1
	vxorps	4664(%rsp), %ymm0, %ymm0
	vxorps	4696(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2600(%rsp)
	vmovups	%ymm1, 2632(%rsp)
	vmovups	3696(%rsp), %ymm0
	vmovups	3728(%rsp), %ymm1
	vxorps	4728(%rsp), %ymm0, %ymm0
	vxorps	4760(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2664(%rsp)
	vmovups	%ymm1, 2696(%rsp)
	vmovups	3760(%rsp), %ymm0
	vmovups	3792(%rsp), %ymm1
	vxorps	4792(%rsp), %ymm0, %ymm0
	vxorps	4824(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2728(%rsp)
	vmovups	%ymm1, 2760(%rsp)
	vmovups	3824(%rsp), %ymm0
	vmovups	3856(%rsp), %ymm1
	vxorps	4856(%rsp), %ymm0, %ymm0
	vxorps	4888(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2792(%rsp)
	vmovups	%ymm1, 2824(%rsp)
	vmovups	3888(%rsp), %ymm0
	vmovups	3920(%rsp), %ymm1
	vxorps	4920(%rsp), %ymm0, %ymm0
	vxorps	4952(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2856(%rsp)
	vmovups	%ymm1, 2888(%rsp)
	vmovups	3952(%rsp), %ymm0
	vmovups	3984(%rsp), %ymm1
	vxorps	4984(%rsp), %ymm0, %ymm0
	vxorps	5016(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2920(%rsp)
	vmovups	%ymm1, 2952(%rsp)
	vmovups	4016(%rsp), %ymm0
	vmovups	4048(%rsp), %ymm1
	vxorps	5048(%rsp), %ymm0, %ymm0
	vxorps	5080(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 2984(%rsp)
	vmovups	%ymm1, 3016(%rsp)
	vmovups	4080(%rsp), %ymm0
	vmovups	4112(%rsp), %ymm1
	vxorps	5112(%rsp), %ymm0, %ymm0
	vxorps	5144(%rsp), %ymm1, %ymm1
	vmovups	%ymm0, 3048(%rsp)
	vmovups	%ymm1, 3080(%rsp)
	vmovdqa	.LCPI43_0(%rip), %ymm0          # ymm0 = [14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,30,31,28,29,26,27,24,25,22,23,20,21,18,19,16,17]
	vmovdqu	c(%rsi,%rsi), %ymm1
	vmovdqu	c+32(%rsi,%rsi), %ymm2
	vmovdqu	c+64(%rsi,%rsi), %ymm3
	vmovdqu	c+96(%rsi,%rsi), %ymm4
	vpshufb	%ymm0, %ymm1, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -30(%rbx,%rdx,2)
	vpshufb	%ymm0, %ymm2, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -62(%rbx,%rdx,2)
	vpshufb	%ymm0, %ymm3, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -94(%rbx,%rdx,2)
	vpshufb	%ymm0, %ymm4, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -126(%rbx,%rdx,2)
	vpbroadcastd	%xmm0, %ymm0
	vpbroadcastd	.LCPI44_0(%rip), %ymm1  # ymm1 = [1,1,1,1,1,1,1,1]
	vmovdqa	%ymm1, %ymm2
	vmovdqa	%ymm1, %ymm3
	vmovdqa	%ymm1, %ymm4
	vpbroadcastd	.LCPI44_0(%rip), %ymm1  # ymm1 = [1,1,1,1,1,1,1,1]
	vmovdqa	%ymm1, %ymm2
	vmovdqa	%ymm1, %ymm3
	vmovdqa	%ymm1, %ymm4
	vpmulld	%ymm0, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm3, %ymm3
	vpmulld	%ymm0, %ymm4, %ymm4
	vpmulld	%ymm0, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm3, %ymm3
	vpmulld	%ymm0, %ymm4, %ymm4
	vpmulld	%ymm0, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm3, %ymm3
	vpmulld	%ymm0, %ymm4, %ymm4
	vpmulld	%ymm0, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm3, %ymm3
	vpmulld	%ymm0, %ymm4, %ymm4
	vpmulld	%ymm0, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm2, %ymm2
	vpmulld	%ymm0, %ymm3, %ymm3
	vpmulld	%ymm0, %ymm4, %ymm4
	vpmulld	%ymm4, %ymm2, %ymm0
	vpmulld	%ymm3, %ymm1, %ymm1
	vpmulld	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovdqa	.LCPI45_0(%rip), %ymm0          # ymm0 = [14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,30,31,28,29,26,27,24,25,22,23,20,21,18,19,16,17]
	vmovdqu	c(%rsi,%rsi), %ymm1
	vmovdqu	c+32(%rsi,%rsi), %ymm2
	vmovdqu	c+64(%rsi,%rsi), %ymm3
	vmovdqu	c+96(%rsi,%rsi), %ymm4
	vpshufb	%ymm0, %ymm1, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -22(%rsp,%rdx,2)
	vpshufb	%ymm0, %ymm2, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -54(%rsp,%rdx,2)
	vpshufb	%ymm0, %ymm3, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -86(%rsp,%rdx,2)
	vpshufb	%ymm0, %ymm4, %ymm1
	vpermq	$78, %ymm1, %ymm1               # ymm1 = ymm1[2,3,0,1]
	vmovdqu	%ymm1, -118(%rsp,%rdx,2)
	vmovdqu	%ymm0, mkpol.g+480(%rip)
	vmovdqu	%ymm0, mkpol.g+448(%rip)
	vmovdqu	%ymm0, mkpol.g+416(%rip)
	vmovdqu	%ymm0, mkpol.g+384(%rip)
	vmovdqu	%ymm0, mkpol.g+352(%rip)
	vmovdqu	%ymm0, mkpol.g+320(%rip)
	vmovdqu	%ymm0, mkpol.g+288(%rip)
	vmovdqu	%ymm0, mkpol.g+256(%rip)
	vmovdqu	%ymm0, mkpol.g+224(%rip)
	vmovdqu	%ymm0, mkpol.g+192(%rip)
	vmovdqu	%ymm0, mkpol.g+160(%rip)
	vmovdqu	%ymm0, mkpol.g+128(%rip)
	vmovdqu	%ymm0, mkpol.g+96(%rip)
	vmovdqu	%ymm0, mkpol.g+64(%rip)
	vmovdqu	%ymm0, mkpol.g+32(%rip)
	vmovdqu	%ymm0, mkpol.g(%rip)
	vmovdqu	mkpol.g+482(%rip), %ymm0
	vpshufb	.LCPI46_0(%rip), %ymm0, %ymm0   # ymm0 = ymm0[14,15,12,13,10,11,8,9,6,7,4,5,2,3,0,1,30,31,28,29,26,27,24,25,22,23,20,21,18,19,16,17]
	vpermq	$78, %ymm0, %ymm0               # ymm0 = ymm0[2,3,0,1]
	vmovdqu	%ymm0, 16(%rsp)
	vpmovzxwd	%xmm0, %ymm0            # ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
	vpmovzxwd	%xmm1, %ymm1            # ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpaddd	%ymm2, %ymm0, %ymm0
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpaddd	%ymm2, %ymm1, %ymm1
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm3, %ymm0, %ymm0
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpaddd	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm8
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpaddd	%ymm3, %ymm0, %ymm9
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpaddd	%ymm3, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm8, %ymm1
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpaddd	%ymm5, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm9, %ymm8
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpaddd	%ymm2, %ymm3, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm1
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpmovzxwd	%xmm3, %ymm3            # ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpaddd	%ymm5, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm2, %ymm2
	vpmovzxwd	%xmm4, %ymm4            # ymm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero,xmm4[4],zero,xmm4[5],zero,xmm4[6],zero,xmm4[7],zero
	vpaddd	%ymm4, %ymm3, %ymm3
	vpmovzxwd	%xmm5, %ymm5            # ymm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero,xmm5[4],zero,xmm5[5],zero,xmm5[6],zero,xmm5[7],zero
	vpaddd	%ymm5, %ymm2, %ymm2
	vpaddd	%ymm2, %ymm8, %ymm0
	vpmovzxwd	%xmm2, %ymm2            # ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
	vpaddd	%ymm2, %ymm3, %ymm2
	vpaddd	%ymm2, %ymm1, %ymm1
	vpaddd	%ymm0, %ymm1, %ymm0
	vextracti128	$1, %ymm0, %xmm1
	vmovups	2082(%rsp,%rdi,2), %ymm0
	vmovups	2114(%rsp,%rdi,2), %ymm1
	vmovups	2146(%rsp,%rdi,2), %ymm2
	vmovups	2178(%rsp,%rdi,2), %ymm3
	vxorps	4146(%rsp,%rdi,2), %ymm0, %ymm0
	vxorps	4178(%rsp,%rdi,2), %ymm1, %ymm1
	vmovups	%ymm0, 2082(%rsp,%rdi,2)
	vmovups	%ymm1, 2114(%rsp,%rdi,2)
	vxorps	4210(%rsp,%rdi,2), %ymm2, %ymm0
	vxorps	4242(%rsp,%rdi,2), %ymm3, %ymm1
	vmovups	%ymm0, 2146(%rsp,%rdi,2)
	vmovups	%ymm1, 2178(%rsp,%rdi,2)
	vmovups	2080(%rsp,%rdi), %ymm0
	vmovups	2112(%rsp,%rdi), %ymm1
	vxorps	4144(%rsp,%rdi), %ymm0, %ymm0
	vxorps	4176(%rsp,%rdi), %ymm1, %ymm1
	vmovups	%ymm0, 2080(%rsp,%rdi)
	vmovups	%ymm1, 2112(%rsp,%rdi)
